#!/usr/bin/env python3
"""
å¼€æºé¡¹ç›®æ½œåŠ›é¢„æµ‹ç³»ç»Ÿ - æŒ‡æ ‡è®¡ç®—ä¸»å…¥å£
åŒ…å«äº”å¤§ç»´åº¦è¯„åˆ†ï¼šç¤¾åŒºæ´»åŠ›ã€ä»£ç å¥åº·ã€ç»´æŠ¤æ•ˆç‡ã€ä¸»é¢˜åˆ›æ–°ã€å¤–éƒ¨å¸å¼•åŠ›
"""

import pandas as pd
import yaml
import sys
import os
import glob
import numpy as np
from datetime import datetime
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/metrics_calculation.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

# å¯¼å…¥æ‰€æœ‰ç»´åº¦è®¡ç®—å™¨
try:
    from src.metrics.community_vitality import CommunityVitalityCalculator
    from src.metrics.code_health import CodeHealthCalculator
    from src.metrics.maintenance_efficiency import MaintenanceEfficiencyCalculator
    from src.metrics.topic_innovation import TopicInnovationCalculator
    from src.metrics.external_appeal import ExternalAppealCalculator

    print("âœ… æˆåŠŸå¯¼å…¥æ‰€æœ‰è®¡ç®—å™¨æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥è®¡ç®—å™¨æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²åˆ›å»ºä»¥ä¸‹æ–‡ä»¶:")
    print("  src/metrics/community_vitality.py")
    print("  src/metrics/code_health.py")
    print("  src/metrics/maintenance_efficiency.py")
    print("  src/metrics/topic_innovation.py")
    print("  src/metrics/external_appeal.py")
    print("  src/metrics/__init__.py")
    sys.exit(1)


def find_data_file():
    """æ™ºèƒ½æŸ¥æ‰¾æ•°æ®æ–‡ä»¶"""

    print("ğŸ” æœç´¢æ•°æ®æ–‡ä»¶...")

    # æŒ‰ä¼˜å…ˆçº§å°è¯•çš„æ–‡ä»¶è·¯å¾„
    search_patterns = [
        "data/processed/cleaned_repositories.csv",  # æ ‡å‡†ä½ç½®
        "cleaned_repositories.csv",  # å½“å‰ç›®å½•
        "data/processed/*clean*.csv",  # åŒ…å«cleanå…³é”®è¯
        "data/processed/*repository*.csv",  # åŒ…å«repositoryå…³é”®è¯
        "data/*clean*.csv",  # dataç›®å½•ä¸‹çš„cleanæ–‡ä»¶
        "../data/processed/cleaned_repositories.csv",  # ä¸Šçº§ç›®å½•çš„æ ‡å‡†ä½ç½®
    ]

    found_files = []

    for pattern in search_patterns:
        if "*" in pattern:
            # ä½¿ç”¨é€šé…ç¬¦æœç´¢
            matches = glob.glob(pattern)
            if matches:
                found_files.extend(matches)
                print(f"  æ‰¾åˆ°åŒ¹é…æ¨¡å¼ {pattern}: {len(matches)} ä¸ªæ–‡ä»¶")
        elif os.path.exists(pattern):
            found_files.append(pattern)
            print(f"  æ‰¾åˆ°æ–‡ä»¶: {pattern}")

    # å»é‡å¹¶æ’åºï¼ˆæŒ‰ä¿®æ”¹æ—¶é—´ï¼Œæœ€æ–°çš„ä¼˜å…ˆï¼‰
    if found_files:
        found_files = list(set(found_files))  # å»é‡
        found_files.sort(key=os.path.getmtime, reverse=True)

        print(f"\nğŸ“ æ‰¾åˆ°ä»¥ä¸‹æ•°æ®æ–‡ä»¶:")
        for i, file in enumerate(found_files[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
            mtime = datetime.fromtimestamp(os.path.getmtime(file))
            size_mb = os.path.getsize(file) / (1024 * 1024)
            print(f"  {i}. {file}")
            print(f"     å¤§å°: {size_mb:.2f} MB, ä¿®æ”¹æ—¶é—´: {mtime:%Y-%m-%d %H:%M}")

        return found_files[0]

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ—å‡ºå½“å‰ç›®å½•ç»“æ„
    print("\nâŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
    print("\nå½“å‰ç›®å½•ç»“æ„:")
    for root, dirs, files in os.walk("."):
        level = root.replace(".", "").count(os.sep)
        if level > 2:  # é™åˆ¶æ·±åº¦
            continue
        indent = " " * 4 * level
        dir_name = os.path.basename(root) if root != "." else "."
        print(f"{indent}{dir_name}/")

        # æ˜¾ç¤ºCSVæ–‡ä»¶
        subindent = " " * 4 * (level + 1)
        csv_files = [f for f in files if f.endswith(".csv")]
        for file in csv_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"{subindent}{file}")

    return None


class MetricsPipeline:
    """æŒ‡æ ‡è®¡ç®—æµæ°´çº¿"""

    def __init__(self, config_path='config/metrics_config.yaml'):
        self.config = self._load_config(config_path)
        self.calculators = self._initialize_calculators()
        logger.info(f"æŒ‡æ ‡è®¡ç®—æµæ°´çº¿åˆå§‹åŒ–å®Œæˆï¼ŒåŒ…å« {len(self.calculators)} ä¸ªç»´åº¦")

    def _load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            # ä½¿ç”¨UTF-8ç¼–ç æ‰“å¼€æ–‡ä»¶
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
            return config
        except FileNotFoundError:
            logger.warning(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._create_default_config()
        except yaml.YAMLError as e:
            logger.error(f"é…ç½®æ–‡ä»¶è§£æé”™è¯¯: {e}")
            print(f"âŒ é…ç½®æ–‡ä»¶è§£æé”™è¯¯: {e}")
            return self._create_default_config()
        except UnicodeDecodeError as e:
            logger.error(f"é…ç½®æ–‡ä»¶ç¼–ç é”™è¯¯: {e}")
            print(f"âŒ é…ç½®æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨GBKç¼–ç ...")
            # å°è¯•ä½¿ç”¨GBKç¼–ç 
            try:
                with open(config_path, 'r', encoding='gbk') as f:
                    config = yaml.safe_load(f)
                logger.info(f"é…ç½®æ–‡ä»¶ä½¿ç”¨GBKç¼–ç åŠ è½½æˆåŠŸ: {config_path}")
                return config
            except Exception as e2:
                logger.error(f"ä½¿ç”¨GBKç¼–ç ä¹Ÿå¤±è´¥: {e2}")
                print(f"âŒ ä½¿ç”¨GBKç¼–ç ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return self._create_default_config()

    def _create_default_config(self):
        """åˆ›å»ºé»˜è®¤é…ç½®"""
        default_config = {
            'weights': {
                'community_vitality': 0.25,
                'code_health': 0.20,
                'maintenance_efficiency': 0.15,
                'topic_innovation': 0.20,
                'external_appeal': 0.20
            },
            'normalization': {
                'method': 'minmax',
                'clip_outliers': True,
                'outlier_threshold': 3.0
            },
            'logging': {
                'level': 'INFO',
                'file': 'logs/metrics_calculation.log'
            },
            'output': {
                'save_intermediate': True,
                'generate_report': True,
                'report_format': 'both'  # 'text', 'json', 'both'
            }
        }

        # ç¡®ä¿configç›®å½•å­˜åœ¨
        os.makedirs('config', exist_ok=True)

        config_file = 'config/metrics_config.yaml'
        # ä½¿ç”¨UTF-8ç¼–ç å†™å…¥æ–‡ä»¶
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(default_config, f, default_flow_style=False, allow_unicode=True)

        print("ğŸ“ å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: config/metrics_config.yaml")
        logger.info("å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶")
        return default_config

    def _initialize_calculators(self):
        """åˆå§‹åŒ–æ‰€æœ‰è®¡ç®—å™¨"""
        calculators = {
            'community_vitality': CommunityVitalityCalculator(),
            'code_health': CodeHealthCalculator(),
            'maintenance_efficiency': MaintenanceEfficiencyCalculator(),
            'topic_innovation': TopicInnovationCalculator(),
            'external_appeal': ExternalAppealCalculator()
        }

        print(f"ğŸ› ï¸  å·²åˆå§‹åŒ– {len(calculators)} ä¸ªè®¡ç®—å™¨:")
        for name in calculators.keys():
            print(f"  â€¢ {name.replace('_', ' ').title()}")

        return calculators

    def _display_code_health_details(self, df):
        """æ˜¾ç¤ºä»£ç å¥åº·åº¦è¯¦ç»†ä¿¡æ¯"""
        if 'csq_score' in df.columns and 'epm_score' in df.columns and 'dc_score' in df.columns:
            print(f"      å­ç»´åº¦è¯¦æƒ…:")
            print(f"        ä»£ç ç»“æ„è´¨é‡: {df['csq_score'].mean():.2f}")
            print(f"        å·¥ç¨‹å®è·µæˆç†Ÿåº¦: {df['epm_score'].mean():.2f}")
            print(f"        æ–‡æ¡£å®Œæ•´æ€§: {df['dc_score'].mean():.2f}")

            # è¯†åˆ«å‰3å¤§è¯­è¨€çš„ä»£ç å¥åº·åº¦
            if 'language' in df.columns:
                top_languages = df['language'].value_counts().head(3).index
                for lang in top_languages:
                    if pd.notna(lang):
                        lang_df = df[df['language'] == lang]
                        if len(lang_df) > 0:
                            print(f"        {lang}: {lang_df['code_health_score'].mean():.2f}")

    def _display_community_vitality_details(self, df):
        """æ˜¾ç¤ºç¤¾åŒºæ´»åŠ›è¯¦ç»†ä¿¡æ¯"""
        if 'activity_level_refined' in df.columns:
            print(f"      æ´»è·ƒåº¦åˆ†å¸ƒ:")
            for level in ['éå¸¸æ´»è·ƒ(â‰¤7å¤©)', 'æ´»è·ƒ(8-30å¤©)', 'ä¸€èˆ¬æ´»è·ƒ(31-90å¤©)']:
                if level in df['activity_level_refined'].values:
                    count = (df['activity_level_refined'] == level).sum()
                    pct = count / len(df) * 100
                    print(f"        {level}: {count}ä¸ªé¡¹ç›® ({pct:.1f}%)")

    def _display_maintenance_efficiency_details(self, df):
        """æ˜¾ç¤ºç»´æŠ¤æ•ˆç‡è¯¦ç»†ä¿¡æ¯"""
        if 'ise_score' in df.columns and 'crq_score' in df.columns and 'csd_score' in df.columns:
            print(f"      å­ç»´åº¦è¯¦æƒ…:")
            print(f"        é—®é¢˜è§£å†³æ•ˆç‡: {df['ise_score'].mean():.2f}")
            print(f"        ä»£ç å®¡æŸ¥è´¨é‡: {df['crq_score'].mean():.2f}")
            print(f"        åä½œè§„èŒƒåŒ–: {df['csd_score'].mean():.2f}")

            # åˆ†æé—®é¢˜è§£å†³æ—¶é—´çš„åˆ†å¸ƒ
            if 'avg_issue_close_time_days' in df.columns:
                median_close_time = df['avg_issue_close_time_days'].median()
                print(f"        é—®é¢˜å¹³å‡è§£å†³æ—¶é—´: {median_close_time:.1f}å¤©")

            # åˆ†æPRåˆå¹¶ç‡çš„åˆ†å¸ƒ
            if 'pr_acceptance_rate' in df.columns:
                avg_acceptance_rate = df['pr_acceptance_rate'].mean() * 100
                print(f"        PRå¹³å‡æ¥å—ç‡: {avg_acceptance_rate:.1f}%")

    def _display_topic_innovation_details(self, df):
        """æ˜¾ç¤ºä¸»é¢˜åˆ›æ–°åº¦è¯¦ç»†ä¿¡æ¯"""
        if 'tc_score' in df.columns and 'ti_score' in df.columns and 'mdm_score' in df.columns:
            print(f"      å­ç»´åº¦è¯¦æƒ…:")
            print(f"        ä¸»é¢˜é›†ä¸­åº¦: {df['tc_score'].mean():.1f}")
            print(f"        æŠ€æœ¯åˆ›æ–°æ€§: {df['ti_score'].mean():.3f}")
            print(f"        å¸‚åœºéœ€æ±‚å¥‘åˆåº¦: {df['mdm_score'].mean():.3f}")

            # æ˜¾ç¤ºä¸»é¢˜åˆ†æç»“æœ
            if 'num_topics' in df.columns:
                print(f"        å¹³å‡ä¸»é¢˜æ•°: {df['num_topics'].mean():.1f}")

            if 'main_topic' in df.columns and df['main_topic'].notna().any():
                # ç»Ÿè®¡æœ€å¸¸è§çš„ä¸»é¢˜
                topic_counts = df['main_topic'].value_counts().head(3)
                print(f"        æœ€å¸¸è§ä¸»é¢˜:")
                for topic, count in topic_counts.items():
                    pct = count / len(df) * 100
                    print(f"          â€¢ {topic}: {count}ä¸ªé¡¹ç›® ({pct:.1f}%)")

    def _display_external_appeal_details(self, df):
        """æ˜¾ç¤ºå¤–éƒ¨å¸å¼•åŠ›è¯¦ç»†ä¿¡æ¯"""
        if 'gr_score' in df.columns and 'vi_score' in df.columns and 'net_score' in df.columns:
            print(f"      å­ç»´åº¦è¯¦æƒ…:")
            print(f"        å¢é•¿åŠ¿å¤´: {df['gr_score'].mean():.2f}")
            print(f"        å¯è§æ€§: {df['vi_score'].mean():.2f}")
            print(f"        ç½‘ç»œæ•ˆåº”: {df['net_score'].mean():.2f}")

            # æ˜¾ç¤ºå¢é•¿è¶‹åŠ¿
            if 'star_growth_rate' in df.columns:
                avg_growth = df['star_growth_rate'].mean() * 100
                print(f"        Starå¹³å‡å¢é•¿ç‡: {avg_growth:.2f}%")

            # æ˜¾ç¤ºç½‘ç»œæ•ˆåº”æŒ‡æ ‡
            if 'dependents_count' in df.columns:
                median_dependents = df['dependents_count'].median()
                print(f"        è¢«ä¾èµ–æ•°ä¸­ä½æ•°: {median_dependents:.0f}")

    def run(self, input_file, output_file=None):
        """è¿è¡ŒæŒ‡æ ‡è®¡ç®—æµæ°´çº¿"""

        print("=" * 60)
        print("ğŸš€ å¼€å§‹è®¡ç®—å¼€æºé¡¹ç›®æ½œåŠ›è¯„åˆ†")
        print(f"   è¾“å…¥æ–‡ä»¶: {input_file}")
        print("=" * 60)

        # 1. åŠ è½½æ•°æ®
        print("\nğŸ“‚ åŠ è½½æ•°æ®...")
        try:
            # å°è¯•å¤šç§ç¼–ç è¯»å–CSVæ–‡ä»¶
            encodings_to_try = ['utf-8', 'gbk', 'utf-8-sig', 'latin1', 'cp1252']
            df = None

            for encoding in encodings_to_try:
                try:
                    df = pd.read_csv(input_file, parse_dates=['created_at', 'updated_at', 'pushed_at'],
                                     encoding=encoding)
                    print(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸåŠ è½½æ•°æ®")
                    break
                except (UnicodeDecodeError, LookupError) as e:
                    print(f"âš ï¸  å°è¯• {encoding} ç¼–ç å¤±è´¥: {e}")
                    continue
                except Exception as e:
                    # å¯èƒ½æ˜¯å…¶ä»–é”™è¯¯ï¼Œç»§ç»­å°è¯•
                    continue

            if df is None:
                print(f"âŒ æ‰€æœ‰ç¼–ç å°è¯•éƒ½å¤±è´¥ï¼Œæ— æ³•è¯»å–æ–‡ä»¶")
                return None

            print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} ä¸ªé¡¹ç›®ï¼Œ{len(df.columns)} ä¸ªç‰¹å¾")

            # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
            if 'created_at' in df.columns:
                print(f"   æ—¶é—´èŒƒå›´: {df['created_at'].min().date()} è‡³ {df['created_at'].max().date()}")

            if 'language' in df.columns:
                print(f"   è¯­è¨€ç§ç±»: {df['language'].nunique()} ç§")

            if 'stargazers_count' in df.columns:
                print(f"   Starä¸­ä½æ•°: {df['stargazers_count'].median()}")

            logger.info(f"æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} ä¸ªé¡¹ç›®")

        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            print("\nå¯èƒ½çš„åŸå› :")
            print("  1. æ–‡ä»¶è·¯å¾„ä¸æ­£ç¡®")
            print("  2. æ–‡ä»¶æ ¼å¼ä¸æ˜¯æœ‰æ•ˆçš„CSV")
            print("  3. ç¼ºå°‘å¿…è¦çš„åˆ—ï¼ˆå¦‚ created_at, pushed_at ç­‰ï¼‰")
            print("  4. æ–‡ä»¶ç¼–ç é—®é¢˜")
            return None

        # 2. æŒ‰é¡ºåºè®¡ç®—å„ä¸ªç»´åº¦
        print("\n" + "=" * 60)
        print("ğŸ“Š å¼€å§‹è®¡ç®—å„ç»´åº¦è¯„åˆ†")
        print("=" * 60)

        results = {}
        dimension_order = list(self.calculators.keys())

        for i, dim_name in enumerate(dimension_order, 1):
            calculator = self.calculators[dim_name]

            print(f"\n[{i}/{len(dimension_order)}] ğŸ“ˆ è®¡ç®—ç»´åº¦: {dim_name.replace('_', ' ').title()}")
            print(f"   è®¡ç®—å™¨: {calculator.__class__.__name__}")

            try:
                # æ‰§è¡Œè®¡ç®—
                start_time = datetime.now()
                df = calculator.calculate(df)
                elapsed_time = (datetime.now() - start_time).total_seconds()

                # è·å–ç»“æœåˆ—å
                score_column = f'{dim_name}_score'
                if score_column in df.columns:
                    avg_score = df[score_column].mean()
                    results[dim_name] = {
                        'average_score': avg_score,
                        'min_score': df[score_column].min(),
                        'max_score': df[score_column].max(),
                        'std_score': df[score_column].std()
                    }

                    print(f"   âœ… è®¡ç®—å®Œæˆ (è€—æ—¶: {elapsed_time:.2f}ç§’)")
                    print(f"      å¹³å‡åˆ†: {avg_score:.2f}")
                    print(f"      èŒƒå›´: {df[score_column].min():.2f} - {df[score_column].max():.2f}")

                    # å¯¹äºæŸäº›ç»´åº¦ï¼Œæ˜¾ç¤ºé¢å¤–ä¿¡æ¯
                    if dim_name == 'code_health':
                        self._display_code_health_details(df)
                    elif dim_name == 'community_vitality':
                        self._display_community_vitality_details(df)
                    elif dim_name == 'maintenance_efficiency':
                        self._display_maintenance_efficiency_details(df)
                    elif dim_name == 'topic_innovation':
                        self._display_topic_innovation_details(df)
                    elif dim_name == 'external_appeal':
                        self._display_external_appeal_details(df)

                    # ä¿å­˜ä¸­é—´ç»“æœï¼ˆå¦‚æœé…ç½®å…è®¸ï¼‰
                    if self.config.get('output', {}).get('save_intermediate', True):
                        intermediate_file = f"data/processed/intermediate_{dim_name}_scores.csv"
                        intermediate_df = df[['full_name', score_column]].copy()
                        # ä½¿ç”¨UTF-8-sigç¼–ç ä¿å­˜ï¼Œå…¼å®¹Excel
                        intermediate_df.to_csv(intermediate_file, index=False, encoding='utf-8-sig')
                        print(f"      ä¸­é—´ç»“æœä¿å­˜è‡³: {intermediate_file}")

                else:
                    print(f"   âš ï¸  è®¡ç®—å®Œæˆä½†æœªæ‰¾åˆ° {score_column} åˆ—")
                    logger.warning(f"ç»´åº¦ {dim_name} è®¡ç®—å®Œæˆä½†æœªæ‰¾åˆ° {score_column} åˆ—")

            except Exception as e:
                logger.error(f"ç»´åº¦ {dim_name} è®¡ç®—å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
                print(f"   âŒ è®¡ç®—å¤±è´¥: {type(e).__name__}: {e}")
                import traceback
                traceback.print_exc()
                # ç»§ç»­æ‰§è¡Œå…¶ä»–ç»´åº¦

        # 3. è®¡ç®—ç»¼åˆæ½œåŠ›åˆ†ï¼ˆå½“è‡³å°‘æœ‰ä¸¤ä¸ªç»´åº¦å®Œæˆæ—¶ï¼‰
        print("\n" + "=" * 60)
        print("ğŸ§® è®¡ç®—ç»¼åˆæ½œåŠ›åˆ†")
        print("=" * 60)

        completed_dimensions = [dim for dim in dimension_order if f'{dim}_score' in df.columns]

        # åœ¨ run() æ–¹æ³•ä¸­ä¿®æ”¹ç»¼åˆæ½œåŠ›åˆ†è®¡ç®—éƒ¨åˆ†ï¼š
        if len(completed_dimensions) >= 2:
            df['overall_potential_score'] = 0
            total_weight = 0

            for dim_name in completed_dimensions:
                weight = self.config['weights'].get(dim_name, 0.2)
                score_column = f'{dim_name}_score'

                if score_column in df.columns:
                    # å½’ä¸€åŒ–æ‰€æœ‰ç»´åº¦åˆ†æ•°åˆ°0-1èŒƒå›´
                    score = df[score_column].copy()

                    # æ ¹æ®ç»´åº¦ç±»å‹è¿›è¡Œå½’ä¸€åŒ–
                    if dim_name in ['community_vitality', 'code_health', 'maintenance_efficiency']:
                        # è¿™äº›ç»´åº¦å·²ç»æ˜¯0-100åˆ†åˆ¶
                        normalized_score = score / 100.0
                    elif dim_name in ['topic_innovation', 'external_appeal']:
                        # è¿™äº›ç»´åº¦å·²ç»æ˜¯0-1åˆ†åˆ¶
                        normalized_score = score
                    else:
                        # é»˜è®¤å½’ä¸€åŒ–åˆ°0-1
                        min_val = score.min()
                        max_val = score.max()
                        if max_val > min_val:
                            normalized_score = (score - min_val) / (max_val - min_val)
                        else:
                            normalized_score = 0.5

                    df['overall_potential_score'] += weight * normalized_score
                    total_weight += weight

            # å½’ä¸€åŒ–åˆ°0-100èŒƒå›´
            if total_weight > 0:
                df['overall_potential_score'] = (df['overall_potential_score'] / total_weight) * 100

            # ç¡®ä¿åˆ†æ•°åœ¨0-100èŒƒå›´å†…
            df['overall_potential_score'] = np.clip(df['overall_potential_score'], 0, 100)

            print(f"âœ… ç»¼åˆæ½œåŠ›åˆ†è®¡ç®—å®Œæˆ")
            print(f"   åŸºäºç»´åº¦: {', '.join(completed_dimensions)}")
            print(f"   å¹³å‡ç»¼åˆåˆ†: {df['overall_potential_score'].mean():.2f}")
            print(f"   èŒƒå›´: {df['overall_potential_score'].min():.2f} - {df['overall_potential_score'].max():.2f}")

            # æ·»åŠ ç»¼åˆæ½œåŠ›ç­‰çº§
            df['overall_potential_level'] = pd.cut(
                df['overall_potential_score'],
                bins=[0, 20, 40, 60, 80, 100],
                labels=['æ½œåŠ›ä½', 'æ½œåŠ›ä¸€èˆ¬', 'æ½œåŠ›ä¸­ç­‰', 'æ½œåŠ›é«˜', 'æ½œåŠ›å¾ˆé«˜']
            )
        else:
            print(f"âš ï¸  å®Œæˆç»´åº¦ä¸è¶³ï¼ˆ{len(completed_dimensions)}ä¸ªï¼‰ï¼Œæ— æ³•è®¡ç®—ç»¼åˆåˆ†")
            print(f"   éœ€è¦è‡³å°‘2ä¸ªç»´åº¦ï¼Œå½“å‰å®Œæˆ: {completed_dimensions}")

        # 4. ä¿å­˜æœ€ç»ˆç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ’¾ ä¿å­˜ç»“æœ")
        print("=" * 60)

        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"data/processed/scored_repositories_{timestamp}.csv"

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        # é€‰æ‹©è¦ä¿å­˜çš„åˆ—ï¼ˆé¿å…æ•°æ®è¿‡å¤§ï¼‰
        columns_to_save = ['full_name', 'language', 'description', 'stargazers_count',
                           'created_at', 'updated_at', 'pushed_at']

        # æ·»åŠ æ‰€æœ‰è®¡ç®—å¾—åˆ°çš„åˆ†æ•°åˆ—
        score_columns = [col for col in df.columns if col.endswith('_score')]
        columns_to_save.extend(score_columns)

        # æ·»åŠ ä¸»é¢˜å…³é”®è¯åˆ—
        if 'topic_keywords' in df.columns:
            columns_to_save.append('topic_keywords')
        if 'main_topic' in df.columns:
            columns_to_save.append('main_topic')
        if 'num_topics' in df.columns:
            columns_to_save.append('num_topics')
        if 'topic_entropy' in df.columns:
            columns_to_save.append('topic_entropy')

        # æ·»åŠ å…¶ä»–æœ‰ç”¨çš„åˆ—
        other_columns = ['overall_potential_score', 'overall_potential_level']
        columns_to_save.extend([col for col in other_columns if col in df.columns])

        # ç¡®ä¿åˆ—å­˜åœ¨
        columns_to_save = [col for col in columns_to_save if col in df.columns]

        # ä¿å­˜æ•°æ®ï¼Œä½¿ç”¨UTF-8-sigç¼–ç ï¼ˆå…¼å®¹Excelï¼‰
        df[columns_to_save].to_csv(output_file, index=False, encoding='utf-8-sig')

        file_size_mb = os.path.getsize(output_file) / (1024 * 1024)
        print(f"âœ… ç»“æœä¿å­˜è‡³: {output_file}")
        print(f"   æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")
        print(f"   åŒ…å«åˆ—æ•°: {len(columns_to_save)}")

        # 5. ç”ŸæˆæŠ¥å‘Š
        if self.config.get('output', {}).get('generate_report', True):
            self._generate_report(df, results, output_file, completed_dimensions)

        logger.info(f"æŒ‡æ ‡è®¡ç®—æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼Œç»“æœä¿å­˜è‡³: {output_file}")
        return df

    def _generate_report(self, df, results, output_file, completed_dimensions):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“ˆ æŒ‡æ ‡è®¡ç®—åˆ†ææŠ¥å‘Š")
        print("=" * 60)

        print(f"\nğŸ“Š æ•°æ®é›†æ¦‚å†µ:")
        print(f"   é¡¹ç›®æ€»æ•°: {len(df)}")

        if 'created_at' in df.columns:
            try:
                print(f"   æ—¶é—´èŒƒå›´: {df['created_at'].min().date()} è‡³ {df['created_at'].max().date()}")

                avg_age_days = (pd.Timestamp.now() - df['created_at']).dt.days.mean()
                avg_age_months = avg_age_days / 30.44
                print(f"   å¹³å‡é¡¹ç›®å¹´é¾„: {avg_age_months:.1f} ä¸ªæœˆ ({avg_age_days:.0f} å¤©)")
            except:
                pass

        # å„ç»´åº¦è¯„åˆ†ç»Ÿè®¡
        print(f"\nğŸ“‹ å„ç»´åº¦è¯„åˆ†ç»Ÿè®¡:")
        for dim_name, stats in results.items():
            dim_display = dim_name.replace('_', ' ').title()
            print(f"   {dim_display}:")
            print(f"     å¹³å‡åˆ†: {stats['average_score']:.2f}")
            print(f"     èŒƒå›´: {stats['min_score']:.2f} - {stats['max_score']:.2f}")
            print(f"     æ ‡å‡†å·®: {stats['std_score']:.2f}")

        # ç»¼åˆæ½œåŠ›æ’å
        if 'overall_potential_score' in df.columns:
            print(f"\nğŸ† ç»¼åˆæ½œåŠ›æ’åå‰10:")
            top_10 = df.nlargest(10, 'overall_potential_score')[
                ['full_name', 'language', 'stargazers_count', 'overall_potential_score']
            ]

            for i, (_, row) in enumerate(top_10.iterrows(), 1):
                # ç¡®ä¿æ˜¾ç¤ºå®Œæ•´é¡¹ç›®å
                name = str(row['full_name'])
                if len(name) > 40:
                    name = name[:37] + "..."

                lang = str(row['language']) if pd.notna(row['language']) else "Unknown"
                stars = int(row['stargazers_count'])
                score = row['overall_potential_score']

                print(f"   {i:2d}. {name:<40} {lang:>12} {stars:>5}â­ {score:>6.2f}åˆ†")

            # å„è¯­è¨€è¡¨ç°
            if 'language' in df.columns:
                print(f"\nğŸŒ å„è¯­è¨€è¡¨ç°ï¼ˆåŸºäºç»¼åˆæ½œåŠ›åˆ†ï¼‰:")
                lang_stats = []
                for lang in df['language'].unique():
                    if pd.notna(lang) and lang != 'Unknown':
                        lang_df = df[df['language'] == lang]
                        if len(lang_df) >= 3:  # è‡³å°‘æœ‰3ä¸ªé¡¹ç›®æ‰ç»Ÿè®¡
                            avg_score = lang_df['overall_potential_score'].mean()
                            lang_stats.append((lang, avg_score, len(lang_df)))

                # æŒ‰å¹³å‡åˆ†æ’åº
                lang_stats.sort(key=lambda x: x[1], reverse=True)

                for i, (lang, avg_score, count) in enumerate(lang_stats[:5], 1):
                    print(f"   {i:2d}. {lang:<15} {avg_score:>6.2f}åˆ† ({count:>3}ä¸ªé¡¹ç›®)")

        # æ”¹è¿›å»ºè®®
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")

        # æ£€æŸ¥æ•°æ®ç¼ºå¤±
        missing_fields = []
        for field in ['language', 'description', 'topics']:
            if field in df.columns and df[field].isnull().mean() > 0.1:
                missing_pct = df[field].isnull().mean() * 100
                missing_fields.append(f"{field} ({missing_pct:.1f}%ç¼ºå¤±)")

        if missing_fields:
            print(f"   1. æ•°æ®è´¨é‡: éœ€è¦å¤„ç†ç¼ºå¤±å­—æ®µ - {', '.join(missing_fields)}")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´å¤šæ•°æ®
        if len(completed_dimensions) < 5:
            print(f"   2. ç»´åº¦è¦†ç›–: å½“å‰å®Œæˆ{len(completed_dimensions)}/5ä¸ªç»´åº¦")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¹³è¡¡é‡‡æ ·
        if 'language' in df.columns:
            lang_counts = df['language'].value_counts()
            if len(lang_counts) > 0:
                top_lang_pct = lang_counts.iloc[0] / len(df) * 100
                if top_lang_pct > 50:
                    print(f"   3. æ ·æœ¬å¹³è¡¡: {lang_counts.index[0]}å æ¯”è¿‡é«˜ ({top_lang_pct:.1f}%)ï¼Œå»ºè®®è¡¥å……å…¶ä»–è¯­è¨€é¡¹ç›®")

        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®:")
        print(f"   å®Œæ•´æ•°æ®: {output_file}")

        # åˆ—å‡ºæ‰€æœ‰ä¸­é—´æ–‡ä»¶
        intermediate_files = glob.glob("data/processed/intermediate_*.csv")
        if intermediate_files:
            print(f"   ä¸­é—´æ–‡ä»¶:")
            for file in intermediate_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                try:
                    file_size_kb = os.path.getsize(file) / 1024
                    print(f"     â€¢ {os.path.basename(file)} ({file_size_kb:.1f} KB)")
                except:
                    print(f"     â€¢ {os.path.basename(file)}")


def main():
    """ä¸»ç¨‹åºå…¥å£"""

    print("=" * 70)
    print("ğŸ“Š å¼€æºé¡¹ç›®æ½œåŠ›é¢„æµ‹ç³»ç»Ÿ - æŒ‡æ ‡è®¡ç®—æ¨¡å—")
    print("=" * 70)

    # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    data_file = find_data_file()

    if data_file is None:
        print("\nâŒ æ— æ³•è‡ªåŠ¨æ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        print("\nè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œä¹‹ä¸€:")
        print("  1. å°†æ•°æ®æ–‡ä»¶æ”¾å…¥ data/processed/ ç›®å½•")
        print("  2. ä¿®æ”¹ä¸‹é¢çš„ manual_file_path å˜é‡æŒ‡å®šæ–‡ä»¶è·¯å¾„")

        # æ‰‹åŠ¨æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœéœ€è¦ï¼‰
        manual_file_path = "data/processed/cleaned_repositories.csv"

        if os.path.exists(manual_file_path):
            print(f"\nâœ… ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šçš„æ–‡ä»¶: {manual_file_path}")
            data_file = manual_file_path
        else:
            print(f"\nâŒ æ‰‹åŠ¨æŒ‡å®šçš„æ–‡ä»¶ä¹Ÿä¸å­˜åœ¨: {manual_file_path}")
            print("\nè¯·ç¡®ä¿:")
            print("  â€¢ æ•°æ®æ–‡ä»¶å·²æ­£ç¡®æ”¾ç½®")
            print("  â€¢ æ–‡ä»¶åŒ…å«ä»¥ä¸‹åˆ—: created_at, updated_at, pushed_at, language, description, topics")
            sys.exit(1)

    print(f"\nâœ… ä½¿ç”¨æ•°æ®æ–‡ä»¶: {data_file}")

    # åˆ›å»ºå¹¶è¿è¡Œæµæ°´çº¿
    try:
        pipeline = MetricsPipeline()

        print("\n" + "=" * 70)
        print("ğŸš€ å¼€å§‹æŒ‡æ ‡è®¡ç®—æµæ°´çº¿")
        print("=" * 70)

        result_df = pipeline.run(input_file=data_file)

        if result_df is not None:
            print("\n" + "=" * 70)
            print("ğŸ‰ æŒ‡æ ‡è®¡ç®—æµæ°´çº¿æ‰§è¡Œå®Œæˆ!")
            print("=" * 70)

            # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
            total_dimensions = len(
                [col for col in result_df.columns if col.endswith('_score') and not col.startswith('overall')])

            print(f"\nğŸ“ˆ è®¡ç®—å®Œæˆç»Ÿè®¡:")
            print(f"   æ€»é¡¹ç›®æ•°: {len(result_df)}")
            print(f"   å®Œæˆç»´åº¦: {total_dimensions}ä¸ª")

            if 'overall_potential_score' in result_df.columns:
                print(
                    f"   ç»¼åˆæ½œåŠ›åˆ†èŒƒå›´: {result_df['overall_potential_score'].min():.2f} - {result_df['overall_potential_score'].max():.2f}")
                print(f"   ç»¼åˆæ½œåŠ›åˆ†ä¸­ä½æ•°: {result_df['overall_potential_score'].median():.2f}")

                # æ˜¾ç¤ºæ½œåŠ›åˆ†å¸ƒ
                if 'overall_potential_level' in result_df.columns:
                    level_counts = result_df['overall_potential_level'].value_counts()
                    print(f"\nğŸ“Š ç»¼åˆæ½œåŠ›åˆ†å¸ƒ:")
                    for level, count in level_counts.items():
                        pct = count / len(result_df) * 100
                        print(f"   â€¢ {level}: {count}ä¸ªé¡¹ç›® ({pct:.1f}%)")


            # ç”Ÿæˆé…ç½®æ–‡ä»¶è¯´æ˜
            config_file = 'config/metrics_config.yaml'
            if os.path.exists(config_file):
                print(f"\nâš™ï¸  é…ç½®æ–‡ä»¶ä½ç½®: {config_file}")
                print("   å¯ä»¥ä¿®æ”¹æ­¤æ–‡ä»¶è°ƒæ•´å„ç»´åº¦æƒé‡:")
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = yaml.safe_load(f)
                    if config and 'weights' in config:
                        print("   å½“å‰æƒé‡è®¾ç½®:")
                        for dim, weight in config['weights'].items():
                            dim_name = dim.replace('_', ' ').title()
                            print(f"     â€¢ {dim_name}: {weight}")
                except Exception as e:
                    print(f"   æ— æ³•è¯»å–é…ç½®æ–‡ä»¶: {e}")

        else:
            print("\nâŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥")

    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        logger.info("ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logger.error(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {type(e).__name__}: {e}", exc_info=True)
        print(f"\nâŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        # å¦‚æœæœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä½œä¸ºæ•°æ®æ–‡ä»¶è·¯å¾„
        data_file_arg = sys.argv[1]
        if os.path.exists(data_file_arg):
            print(f"ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šçš„æ–‡ä»¶: {data_file_arg}")

            # åˆ›å»ºæµæ°´çº¿å¹¶è¿è¡Œ
            pipeline = MetricsPipeline()
            result_df = pipeline.run(input_file=data_file_arg)
        else:
            print(f"âŒ æŒ‡å®šçš„æ–‡ä»¶ä¸å­˜åœ¨: {data_file_arg}")
            sys.exit(1)
    else:
        # æ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œæ‰§è¡Œä¸»ç¨‹åº
        main()