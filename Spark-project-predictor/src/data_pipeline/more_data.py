import requests
import pandas as pd
import time
import json
from datetime import datetime, timedelta
import sys


# ========== 1. å®‰å…¨çš„åŸºç¡€æœç´¢å‡½æ•° ==========
def safe_github_search(query, max_items=100, token=None):
    """
    å®‰å…¨çš„GitHubæœç´¢å‡½æ•°ï¼Œé¿å…ç¼–ç é—®é¢˜
    ä½¿ç”¨ç›´æ¥APIè°ƒç”¨ï¼Œç®€åŒ–å¤„ç†é€»è¾‘
    """

    # æ¸…ç†æŸ¥è¯¢å­—ç¬¦ä¸²ä¸­çš„éASCIIå­—ç¬¦
    if isinstance(query, str):
        query = ''.join(char for char in query if ord(char) < 128)

    url = "https://api.github.com/search/repositories"

    # å‡†å¤‡è¯·æ±‚å‚æ•°
    params = {
        "q": query,
        "sort": "updated",
        "order": "desc",
        "per_page": min(100, max_items),
        "page": 1
    }

    headers = {
        "Accept": "application/vnd.github.v3+json",
        "User-Agent": "GitHub-Data-Collector"
    }

    # æ·»åŠ tokenï¼ˆå¦‚æœæœ‰ï¼‰
    if token and token.strip():
        safe_token = ''.join(char for char in token.strip() if ord(char) < 128)
        if safe_token:
            headers["Authorization"] = f"token {safe_token}"

    all_items = []
    total_fetched = 0

    while total_fetched < max_items:
        try:
            # å‘é€è¯·æ±‚
            response = requests.get(
                url,
                headers=headers,
                params=params,
                timeout=30
            )

            # æ£€æŸ¥å“åº”
            if response.status_code == 200:
                data = response.json()
                items = data.get("items", [])

                if not items:
                    break  # æ²¡æœ‰æ›´å¤šæ•°æ®

                all_items.extend(items)
                total_fetched += len(items)

                print(f"    å·²è·å– {total_fetched}/{max_items} ä¸ªé¡¹ç›®")

                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µé¢
                if len(items) < params["per_page"]:
                    break

                # å‡†å¤‡ä¸‹ä¸€é¡µ
                params["page"] += 1
                time.sleep(1.5)  # ç¤¼è²Œé—´éš”ï¼Œé¿å…APIé™åˆ¶

            elif response.status_code == 403:
                # å¤„ç†APIé™åˆ¶
                reset_time = response.headers.get("X-RateLimit-Reset")
                if reset_time:
                    wait_seconds = max(10, int(reset_time) - int(time.time()))
                    print(f"    APIé™åˆ¶ï¼Œç­‰å¾… {wait_seconds} ç§’...")
                    time.sleep(wait_seconds)
                    continue
                else:
                    print("    APIé™åˆ¶ï¼Œç­‰å¾…60ç§’...")
                    time.sleep(60)
                    continue

            else:
                print(f"    è¯·æ±‚å¤±è´¥: çŠ¶æ€ç  {response.status_code}")
                break

        except requests.exceptions.RequestException as e:
            print(f"    ç½‘ç»œé”™è¯¯: {e}")
            time.sleep(10)
            continue

        except Exception as e:
            print(f"    æ„å¤–é”™è¯¯: {e}")
            break

    print(f"    æœ¬æ¬¡æœç´¢å®Œæˆï¼Œè·å– {len(all_items)} ä¸ªé¡¹ç›®")
    return all_items[:max_items]


# ========== 2. ä¸»æ‰©å……å‡½æ•° ==========
def expand_with_relaxed_queries(existing_file='balanced_github_repositories.csv',
                                target_size=550,
                                github_token=None):
    """
    ä½¿ç”¨æ”¾å®½çš„æœç´¢æ¡ä»¶æ‰©å……æ•°æ®é›†
    """

    print("=" * 60)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œæ”¾å®½æ¡ä»¶çš„æ•°æ®æ‰©å……")
    print("=" * 60)

    # 1. åŠ è½½ç°æœ‰æ•°æ®
    print("\nğŸ“‚ åŠ è½½ç°æœ‰æ•°æ®...")
    try:
        existing_df = pd.read_csv(existing_file)
        existing_count = len(existing_df)
        print(f"âœ… æˆåŠŸåŠ è½½ {existing_count} ä¸ªé¡¹ç›®")

        # æ˜¾ç¤ºç°æœ‰æ•°æ®çš„åŸºæœ¬ç»Ÿè®¡
        if existing_count > 0:
            if 'stargazers_count' in existing_df.columns:
                zero_star = (existing_df['stargazers_count'] == 0).sum()
                zero_pct = zero_star / existing_count * 100
                print(f"   é›¶Staré¡¹ç›®: {zero_star} ({zero_pct:.1f}%)")

            if 'language' in existing_df.columns:
                lang_count = existing_df['language'].nunique()
                print(f"   è¯­è¨€ç§ç±»: {lang_count}")

    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return None

    # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å……
    if existing_count >= target_size:
        print(f"âœ… å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ {target_size}")
        return existing_df

    needed = target_size - existing_count
    print(f"ğŸ“ˆ éœ€è¦è¡¥å……: {needed} ä¸ªé¡¹ç›®")

    # 3. å‡†å¤‡æ”¾å®½çš„æœç´¢æ¡ä»¶
    print("\nğŸ” å‡†å¤‡æ”¾å®½çš„æœç´¢æ¡ä»¶...")

    # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆæ”¾å®½æ¡ä»¶ï¼‰
    today = datetime.now()

    # æ¡ä»¶1ï¼šæ›´å®½çš„æ—¶é—´èŒƒå›´ï¼ˆ3-24ä¸ªæœˆï¼Œè€Œä¸æ˜¯6-18ä¸ªæœˆï¼‰
    three_months_ago = (today - timedelta(days=90)).strftime('%Y-%m-%d')
    twentyfour_months_ago = (today - timedelta(days=720)).strftime('%Y-%m-%d')

    # æ¡ä»¶2ï¼šæ›´å®½çš„StarèŒƒå›´ï¼ˆ1-1000ï¼Œè€Œä¸æ˜¯1-500ï¼‰
    # æ¡ä»¶3ï¼šæ”¾å®½pushedæ¡ä»¶ï¼ˆæœ€è¿‘6ä¸ªæœˆæœ‰æ›´æ–°ï¼Œè€Œä¸æ˜¯3ä¸ªæœˆï¼‰
    six_months_ago = (today - timedelta(days=180)).strftime('%Y-%m-%d')

    # å®šä¹‰å¤šä¸ªæ”¾å®½çš„æœç´¢æ¡ä»¶
    relaxed_queries = [
        # æŸ¥è¯¢1ï¼šå®½æ—¶é—´èŒƒå›´ï¼Œå®½StarèŒƒå›´ï¼Œå®½æ¾æ´»è·ƒåº¦
        {
            "name": "å®½èŒƒå›´åŸºç¡€æŸ¥è¯¢",
            "query": f"created:{twentyfour_months_ago}..{three_months_ago} stars:1..1000",
            "target": min(200, needed + 50)
        },

        # æŸ¥è¯¢2ï¼šèšç„¦è¿‘æœŸé¡¹ç›®ï¼Œä½†StarèŒƒå›´æ›´å®½
        {
            "name": "è¿‘æœŸé¡¹ç›®å®½StarèŒƒå›´",
            "query": f"created:{three_months_ago}..now stars:1..800 pushed:>{six_months_ago}",
            "target": min(150, max(50, needed // 2))
        },

        # æŸ¥è¯¢3ï¼šæŒ‰è¯­è¨€åˆ†ç»„æŸ¥è¯¢ï¼ˆæ”¾å®½æ¡ä»¶ï¼‰
        {
            "name": "Pythonæ”¾å®½æ¡ä»¶",
            "query": f"language:python created:{twentyfour_months_ago}..now stars:1..700",
            "target": min(100, max(30, needed // 3))
        },

        {
            "name": "JavaScriptæ”¾å®½æ¡ä»¶",
            "query": f"language:javascript created:{twentyfour_months_ago}..now stars:1..600",
            "target": min(100, max(30, needed // 3))
        },

        # æŸ¥è¯¢4ï¼šå®Œå…¨ä¸é™åˆ¶è¯­è¨€ï¼Œåªé™åˆ¶æ—¶é—´å’ŒStar
        {
            "name": "å…¨è¯­è¨€æ”¾å®½æ¡ä»¶",
            "query": f"created:{twentyfour_months_ago}..{three_months_ago} stars:10..500",
            "target": min(150, needed)
        }
    ]

    # 4. æ‰§è¡Œæœç´¢
    print("\nâš¡ å¼€å§‹æ‰§è¡Œæ”¾å®½æ¡ä»¶çš„æœç´¢...")

    all_new_repos = []
    total_fetched = 0

    for query_info in relaxed_queries:
        name = query_info["name"]
        query = query_info["query"]
        target = query_info["target"]

        if total_fetched >= needed:
            print("âœ… å·²è·å–è¶³å¤Ÿçš„æ–°é¡¹ç›®")
            break

        print(f"\nğŸ“‹ æ‰§è¡ŒæŸ¥è¯¢: {name}")
        print(f"   æŸ¥è¯¢æ¡ä»¶: {query}")
        print(f"   ç›®æ ‡æ•°é‡: {target}")

        # æ‰§è¡Œæœç´¢
        try:
            new_repos = safe_github_search(
                query=query,
                max_items=target,
                token=github_token
            )

            if new_repos:
                all_new_repos.extend(new_repos)
                total_fetched += len(new_repos)
                print(f"   âœ… æˆåŠŸè·å– {len(new_repos)} ä¸ªé¡¹ç›®")
                print(f"   ç´¯è®¡è·å–: {total_fetched}/{needed}")
            else:
                print("   âš ï¸  æœªè·å–åˆ°é¡¹ç›®")

            # æŸ¥è¯¢é—´ä¼‘æ¯
            time.sleep(3)

        except Exception as e:
            print(f"   âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            continue

    print(f"\nğŸ“¦ æœç´¢é˜¶æ®µå®Œæˆ")
    print(f"   æ–°è·å–é¡¹ç›®æ€»æ•°: {len(all_new_repos)}")

    # 5. å¤„ç†æ–°æ•°æ®
    if not all_new_repos:
        print("âš ï¸  æœªè·å–åˆ°æ–°æ•°æ®ï¼Œä½¿ç”¨ç°æœ‰æ•°æ®")
        return existing_df

    # è½¬æ¢ä¸ºDataFrame
    new_data = []
    for repo in all_new_repos:
        try:
            # æå–å…³é”®ä¿¡æ¯
            repo_info = {
                'id': str(repo.get('id', '')),
                'full_name': str(repo.get('full_name', '')),
                'html_url': str(repo.get('html_url', '')),
                'created_at': str(repo.get('created_at', '')),
                'updated_at': str(repo.get('updated_at', '')),
                'pushed_at': str(repo.get('pushed_at', '')),
                'stargazers_count': int(repo.get('stargazers_count', 0)),
                'forks_count': int(repo.get('forks_count', 0)),
                'open_issues_count': int(repo.get('open_issues_count', 0)),
                'language': str(repo.get('language', '')) if repo.get('language') else '',
                'topics': ', '.join([str(t) for t in repo.get('topics', [])][:5]),
                'description': str(repo.get('description', ''))[:200] if repo.get('description') else '',
                'source': 'github_relaxed'
            }
            new_data.append(repo_info)
        except Exception as e:
            print(f"   å¤„ç†ä»“åº“æ•°æ®æ—¶å‡ºé”™: {e}")
            continue

    new_df = pd.DataFrame(new_data)

    # å»é‡ï¼ˆåŸºäºidï¼‰
    if existing_count > 0 and 'id' in existing_df.columns and 'id' in new_df.columns:
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç¡®ä¿ç±»å‹ä¸€è‡´
        existing_ids = set(existing_df['id'].astype(str).tolist())
        new_df['id_str'] = new_df['id'].astype(str)

        # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„é¡¹ç›®
        before_dedup = len(new_df)
        new_df = new_df[~new_df['id_str'].isin(existing_ids)]
        after_dedup = len(new_df)

        if 'id_str' in new_df.columns:
            new_df = new_df.drop(columns=['id_str'])

        print(f"   å»é‡: {before_dedup} â†’ {after_dedup} ä¸ªé¡¹ç›®")

    # 6. åˆå¹¶æ•°æ®
    print("\nğŸ”„ åˆå¹¶æ•°æ®...")

    if len(new_df) > 0:
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        final_count = len(combined_df)

        # ä¿å­˜æ•°æ®
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f'expanded_repositories_{final_count}_{timestamp}.csv'
        combined_df.to_csv(output_file, index=False, encoding='utf-8')

        print(f"âœ… åˆå¹¶å®Œæˆ!")
        print(f"   æœ€ç»ˆé¡¹ç›®æ•°: {final_count}")
        print(f"   ä¿å­˜è‡³: {output_file}")

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æ‰©å……åç»Ÿè®¡:")
        print(f"   Starä¸­ä½æ•°: {combined_df['stargazers_count'].median():.1f}")

        zero_star = (combined_df['stargazers_count'] == 0).sum()
        zero_pct = zero_star / final_count * 100
        print(f"   é›¶Staré¡¹ç›®: {zero_star} ({zero_pct:.1f}%)")

        lang_count = combined_df['language'].nunique()
        print(f"   è¯­è¨€ç§ç±»: {lang_count}")

        # æ˜¾ç¤ºå‰5å¤§è¯­è¨€
        if 'language' in combined_df.columns:
            top_langs = combined_df['language'].value_counts().head(5)
            print(f"\n   å‰5å¤§è¯­è¨€åˆ†å¸ƒ:")
            for lang, count in top_langs.items():
                pct = count / final_count * 100
                lang_display = str(lang)[:20] if pd.notna(lang) else 'Unknown'
                print(f"     {lang_display:<20} {count:>4} ({pct:>5.1f}%)")

        return combined_df
    else:
        print("âš ï¸  æ²¡æœ‰æ–°å¢çš„å”¯ä¸€é¡¹ç›®")
        return existing_df


# ========== 3. è¾…åŠ©å‡½æ•°ï¼šæµ‹è¯•APIè¿æ¥ ==========
def test_github_connection(token=None):
    """æµ‹è¯•GitHub APIè¿æ¥"""

    print("ğŸ§ª æµ‹è¯•GitHub APIè¿æ¥...")

    headers = {
        "Accept": "application/vnd.github.v3+json",
        "User-Agent": "GitHub-Connection-Test"
    }

    if token and token.strip():
        safe_token = ''.join(char for char in token.strip() if ord(char) < 128)
        if safe_token:
            headers["Authorization"] = f"token {safe_token}"

    try:
        # ç®€å•çš„APIè°ƒç”¨æµ‹è¯•
        response = requests.get(
            "https://api.github.com/zen",
            headers=headers,
            timeout=10
        )

        if response.status_code == 200:
            print(f"âœ… GitHub APIè¿æ¥æ­£å¸¸")
            print(f"   çŠ¶æ€: {response.text}")
            return True
        else:
            print(f"âŒ è¿æ¥å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return False

    except Exception as e:
        print(f"âŒ è¿æ¥å¼‚å¸¸: {e}")
        return False


# ========== 4. ä¸»ç¨‹åºå…¥å£ ==========
def main():
    """ä¸»ç¨‹åº"""

    print("=" * 60)
    print("ğŸ“Š GitHubæ–°ç”Ÿä»£é¡¹ç›®æ•°æ®æ‰©å……å·¥å…· (æ”¾å®½æ¡ä»¶ç‰ˆ)")
    print("=" * 60)

    # é…ç½®å‚æ•°
    EXISTING_FILE = "../../../Tree/readme/balanced_github_repositories.csv"  # ä½ çš„ç°æœ‰æ•°æ®æ–‡ä»¶
    TARGET_SIZE = 1000  # ç›®æ ‡é¡¹ç›®æ•°é‡
    GITHUB_TOKEN = "ä»£æ›¿æ¢"  # æ›¿æ¢ä¸ºä½ çš„token

    # æµ‹è¯•è¿æ¥
    print("\n1. æµ‹è¯•APIè¿æ¥...")
    if not test_github_connection(GITHUB_TOKEN):
        print("âš ï¸  APIè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œå°†ç»§ç»­ä½¿ç”¨å…¬å¼€APIï¼ˆé€Ÿç‡é™åˆ¶è¾ƒä½ï¼‰")
        # å¯ä»¥ä¸ä½¿ç”¨tokenï¼Œä½†é€Ÿç‡é™åˆ¶ä¼šæ›´ä¸¥æ ¼
        use_token = None
    else:
        use_token = GITHUB_TOKEN
        print("âœ… ä½¿ç”¨Tokenè¿›è¡ŒAPIè°ƒç”¨")

    print(f"\n2. æ•°æ®æ‰©å……é…ç½®:")
    print(f"   ç°æœ‰æ–‡ä»¶: {EXISTING_FILE}")
    print(f"   ç›®æ ‡æ•°é‡: {TARGET_SIZE}")

    # æ‰§è¡Œæ•°æ®æ‰©å……
    print("\n3. å¼€å§‹æ•°æ®æ‰©å……...")

    try:
        result_df = expand_with_relaxed_queries(
            existing_file=EXISTING_FILE,
            target_size=TARGET_SIZE,
            github_token=use_token
        )

        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("\n" + "=" * 60)
        if result_df is not None:
            final_count = len(result_df)

            if final_count >= TARGET_SIZE:
                print(f"ğŸ‰ æˆåŠŸ! è¾¾åˆ°ç›®æ ‡è§„æ¨¡: {final_count} ä¸ªé¡¹ç›®")
            elif final_count > 400:
                print(f"âœ… éƒ¨åˆ†æˆåŠŸ: {final_count} ä¸ªé¡¹ç›® (æ¥è¿‘ç›®æ ‡)")
            else:
                print(f"âš ï¸  æœªè¾¾é¢„æœŸ: ä»… {final_count} ä¸ªé¡¹ç›®")

            # æä¾›åç»­å»ºè®®
            print(f"\nğŸ’¡ åç»­å»ºè®®:")
            if final_count < 500:
                print(f"   1. å†æ¬¡è¿è¡Œæ­¤è„šæœ¬ï¼Œå¯èƒ½ä¼šè·å–æ›´å¤šé¡¹ç›®")
                print(f"   2. æ£€æŸ¥ç°æœ‰æ•°æ®çš„Staråˆ†å¸ƒï¼Œè°ƒæ•´æœç´¢æ¡ä»¶")
                print(f"   3. è€ƒè™‘ä½¿ç”¨å¤šä¸ªGitHub Tokenè½®æ¢")
            else:
                print(f"   1. æ•°æ®é‡å·²è¶³å¤Ÿï¼Œå¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µåˆ†æ")
                print(f"   2. ä½¿ç”¨EDAè„šæœ¬éªŒè¯æ•°æ®è´¨é‡")
                print(f"   3. å¼€å§‹è®¾è®¡å¤šç»´åº¦è¯„åˆ†æŒ‡æ ‡")

        else:
            print("âŒ æ•°æ®æ‰©å……å¤±è´¥")

    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


# ========== 5. å¿«é€Ÿä½¿ç”¨è¯´æ˜ ==========
if __name__ == "__main__":
    # ä½¿ç”¨è¯´æ˜
    print("=" * 60)
    print("ä½¿ç”¨è¯´æ˜:")
    print("1. ç¡®ä¿ balanced_github_repositories.csv æ–‡ä»¶å­˜åœ¨")
    print("2. å°†ä»£ç ä¸­çš„ GITHUB_TOKEN æ›¿æ¢ä¸ºä½ çš„Personal Access Token")
    print("3. è¿è¡Œæ­¤è„šæœ¬")
    print("=" * 60)

    # ç¡®è®¤æ‰§è¡Œ
    confirm = input("\næ˜¯å¦å¼€å§‹æ‰§è¡Œæ•°æ®æ‰©å……? (y/n): ")

    if confirm.lower() == 'y':
        main()
    else:
        print("å–æ¶ˆæ‰§è¡Œ")